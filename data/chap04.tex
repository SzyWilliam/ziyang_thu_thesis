% !TeX root = ../thuthesis-example.tex

\chapter{集群故障转移和恢复}

故障转移与恢复是构建高可用性和自动容错能力的核心手段。在上一章中，IoTDB 已经能够全面、精准地诊断出集群的故障，接下来，本章将重点讨论如何有针对性地进行故障恢复和自动转移。

本节描述了在故障检测的基础上，写入请求和查询请求的故障自动转移和恢复能力的设计实现。本节首先描述现有的写入和查询流程，给出容错设计的原则和整体框架，

\section{写入和查询流程概述}

\begin{figure}
  \centering
  \includegraphics[width=0.99\linewidth]{c04-write-process.png}
  \caption{IoTDB写入全流程}
  \label{fig:c04-write-process}
\end{figure}

图\ref{fig:c04-write-process}展示了IoTDB现有的写入和查询流程。整个流程涉及三个主要的角色，分别是客户端的会话（Session）、DataNode的协调者（Coordinator）和共识模块的数据副本。Session的主要作用是服务发现、连接状态管理和发送请求，Coordinator负责接受请求并进行处理，共识模块的副本则负责实际数据的写入或者读取。

第一步，客户端可以通过实例化Session对象，和IoTDB服务端的DataNode建立连接，通过SQL等方式发起写入或者查询请求。目前，集群中的所有 DataNode 提供对等的数据写入和查询服务，客户端可以连接至任意 DataNode 发起请求。

第二步，Coordinator会对整个请求进行解析(SQL Parse)。在这个阶段只进行 SQL 语法和格式的校验，确保请求在语法上的正确性。

第三步，Coordinator会进入分析（Analyze）阶段。在本阶段，Coordinator会进行权限校验，确定本次的写入或者查询要访问那些数据，包括识别涉及的时间序列、属性、以及需要查询或写入的数据范围。同时，Coordinator可能需要和ConfigNode 通信，获取最新的集群 Region 分区信息，了解本次访问的数据在哪些 DataNode 上存储。同时，Coordinator也可能需要从其他 DataNode 拉取相关的元数据信息。

第四步，Coordinator根据分析的结果，生成本次请求的逻辑计划，该计划描述了查询或写入操作的逻辑步骤，例如数据的选择、过滤、聚合等，但不涉及具体的物理执行方式。逻辑计划通常以逻辑查询树的形式表示。

第五步，Coordinator在逻辑计划的基础上，考虑到集群中数据的实际分布情况（哪些 Region 存储在哪些 DataNode 上），生成一个分布式执行计划。该计划将逻辑操作分解为可以在不同 DataNode 上并行执行的多个碎片实例 (FragmentInstance)。每个碎片实例负责处理一个特定的数据分区 Region，具备一个ReplicaSet，代表了这个Region的数据副本所在的 所有DataNode。

第六步，Coordinator作为调度器，负责将分布式执行计划中的每个碎片实例调度到相应的 DataNode 上执行。对于某些操作，协调者可能会选择在本地 DataNode 上执行，而对于需要访问其他 DataNode 数据的操作，则会将相应的碎片实例发送到远程 DataNode 执行。如果调度过程中发生错误，协调者可能会尝试重新调度。

第七步，每一个碎片实例最终会被交付给共识层进行执行。共识层会进行相关的读写操作的执行，并且提供数据复制和一致性的保证。以Raft共识协议为例，交给共识层的写入操作会保证被复制给大多数节点执行完成之后才会返回成功写入，交给共识层的读取操作能够保证读到线性一致性的结果。

第八步，共识模块在接收到执行指令后，会调用底层的IoTDB单机存储引擎（采用LSM架构，使用MemTable作为内存结构，使用TsFile\cite{zhao2024apachetsfile}作为持久性外部存储）进行实际的数据写入或读取操作。每个数据副本都由其所在的 DataNode 上的存储引擎进行管理。


\section{写入和查询流程的容错原则和设计}

写入和查询请求的容错原则和设计可以总结为以下几个方面：

1. 利用共识层的多副本能力进行错误转移。当某一个副本失效时，写入和查询请求应当尝试访问其他的健康副本，重试本次的请求直到成功。

2. 利用对副本的知识和集群的已知故障进行最优的执行计划规划和路由规划。如果在规划阶段，已知副本所在的节点出现了进程宕机、服务不可用、网络不可达的情况，那么在计划和规划的阶段就应该避开这些副本，绕开这些故障从而达到最优的执行路由。

3. 一旦发现在规划、执行的阶段中由于故障的原因无法成功完成请求，那么需要尽快通知客户端节点，而不是无用功地在集群内部重试。在通知时需要尽力给出失败的原因和重试的建议，由客户端根据情况决定后续的策略。


\section{协调者规划阶段的容错设计}

规划阶段的容错设计思路是根据集群当前的节点存活情况和副本存活情况，将碎片实例分配到最优的副本上执行。如果找不到任何一个副本来执行这个碎片实例，那么就在规划阶段就报错返回。

\subsection{基于拓扑感知的规划}

IoTDB协调者在进行查询规划和计划生成时，对于根节点碎片实例(Root FragmentInstance)的摆放需要考虑集群的网络拓扑和分区故障。

根节点碎片实例是一种特殊的碎片实例。相较于普通的碎片实例，根节点碎片实例需要负责从其他的分布在不同的存储节点碎片实例中汇聚结果数据，进行最终数据聚合和计算，并将结果发送返回给查询的请求方。由于根节点碎片实例需要和所有的碎片实例进行通信，所以在规划期间，必须保证根节点碎片实例被调度到的节点能够和其他所有碎片实例被调度到的节点的并集联通。


\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{c04-fi-topology-partition.png}
  \caption{网络分区下的查询失败情况}
  \label{fig:fi-topology-partition}
\end{figure}

图\ref{fig:fi-topology-partition}展示了非对称网络分区下，根节点的错误放置导致查询失败的一个例子。在这个查询中，一共有四个碎片实例，分别分散在三个DataNode上。其中第一个、第二个碎片实例被调度到了DataNode2上，第三个碎片实例被调度到了DataNode3上，第四个碎片实例被调度到了DataNode1上。

此时，DataNode2和DataNode3之间恰巧出现了非对称网络分区，此时，如果将本次查询的根节点碎片实例调度到DataNode2上，那么这个根节点碎片实例就不能连接上第三个碎片实例，也无法收集这个实例上的数据，最后导致本次查询计划无法执行，最终失败。

因此，在决定查询计划的规划和根节点碎片实例的放置的时候，我们需要考虑节点之间的拓扑结构。根节点的碎片实例放置的DataNode必须具备和所有的碎片实例所在的DataNode都网络可达的性质。算法\ref{alg:find_candidates}给出了根节点碎片实例的放置算法。


再次我们给出基于网络拓扑分区的root fragment instance的算法描述。
\begin{algorithm}
  \caption{查找DataNode和FragmentInstance候选}
  \label{alg:find_candidates}
  \begin{algorithmic}
  \REQUIRE 集群拓扑结构 (连通图), FragmentInstance列表
  \ENSURE DataNode候选列表, FragmentInstance候选列表
  
  \STATE $DataNodeCandidates \leftarrow \emptyset$
  \STATE $FragmentInstanceCandidates \leftarrow \emptyset$
  
  \FOR{每个 DataNode $node$ 在 集群拓扑结构 中}
      \STATE $isCandidate \leftarrow true$
      \FOR{每个 FragmentInstance $instance$ 在 FragmentInstance列表 中}
          \STATE $foundConnection \leftarrow false$
          \FOR{每个 Replica $replica$ 在 $instance.ReplicaSet$ 中}
              \IF{$node$ 和 $replica$ 在 连通图 中连通}
                  \STATE $foundConnection \leftarrow true$
                  \STATE \textbf{break} \COMMENT{找到一个连接即可}
              \ENDIF
          \ENDFOR
          \IF{$foundConnection = false$}
              \STATE $isCandidate \leftarrow false$
              \STATE \textbf{break} \COMMENT{如果和任何ReplicaSet都无法连通，则不是候选}
          \ENDIF
      \ENDFOR
      \IF{$isCandidate = true$}
          \STATE $DataNodeCandidates \leftarrow DataNodeCandidates \cup \{node\}$
      \ENDIF
  \ENDFOR
  
  \FOR{每个 FragmentInstance $instance$ 在 FragmentInstance列表 中}
      \STATE $isCandidate \leftarrow false$
      \FOR{每个 DataNode $candidate$ 在 $DataNodeCandidates$ 中}
          \FOR{每个 Replica $replica$ 在 $instance.ReplicaSet$ 中}
              \IF{$candidate = replica$}
                  \STATE $isCandidate \leftarrow true$
                  \STATE \textbf{break} \COMMENT{找到一个候选DataNode即可}
              \ENDIF
          \ENDFOR
          \IF{$isCandidate = true$}
              \STATE \textbf{break} \COMMENT{找到一个候选DataNode即可}
          \ENDIF
      \ENDFOR
      \IF{$isCandidate = true$}
          \STATE $FragmentInstanceCandidates \leftarrow FragmentInstanceCandidates \cup \{instance\}$
      \ENDIF
  \ENDFOR
  
  \RETURN $DataNodeCandidates$, $FragmentInstanceCandidates$
  \end{algorithmic}
  \end{algorithm}

上述的拓扑感知算法能够使本次的查询成功完成。
此时，根节点碎片实例将会被调度到DataNode1节点上。由于DataNode1分别和DataNode2跟DataNode3之间能够联通，因此即使出现了非对称网络分区的情况下，位于DataNode1上的根节点碎片实例依然能够成功拉取本次查询规划的所有的碎片实例的数据，成功执行本次的查询。


\subsection{基于副本状态的规划}

在协调者进行查询规划时，查询规划器会从ConfigNode Leader这边加载最新的分区信息表，从而保证每一个碎片实例的副本都是最优规划的副本。该步骤包括以下的算法：

1. 规划器会首先选择主副本来执行写入和查询请求。对于RatisConsensus来说，上层应用只能选择主副本来执行写入请求。对于IoTConsensus和IoTConsensusV2来说，虽然每一个副本都能够执行写入请求，但为了避免冲突、提高系统的吞吐，规划器依然会选择ConfigNode Leader指定的主副本来完成数据的写入。

2. 在主副本发生故障时，共识模块和ConfigNode Leader会共同保证主副本的切换。当主副本切换后，规划器就会将后续的查询请求路由到新的主副本上进行执行，从而完成故障转移和容错。在途的写入和查询请求则通过章节\ref{sec:failover-schedule}所描述的调度阶段的容错设计来实现故障的转移和容错。

3. 如果在规划时期，规划器发现某一个Region的所有副本都不可用，那么会直接使这一次查询失败，并返回失败结果给客户端，由客户端对重试的策略作出最终的决策。
在本文的工作之前，规划器并不会使请求立马失败，而是会尝试在每一个副本之间重试，并在每一次重试中间进行一段时间的等待。这种策略常常能够解决副本组暂时性不可用的情况，但是却在网络分区的情况下会进行大量无谓的等待。

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{c04-write-with-topology.png}
  \caption{网络分区下的写入失败情况}
  \label{fig:c04-write-with-topology}
\end{figure}

图\ref{fig:c04-write-with-topology}中展示了这种大量无谓等待的场景。在本场景中，DataNode3和DataNode1和DataNode2


\section{协调者调度阶段的容错设计}\label{sec:failover-schedule}





\subsection{基于拓扑感知的路由规划算法}

IoTDB现有的查询计划生成和路由选择依赖集群的网络拓扑结构。在非对称网络分区的故障下，IoTDB生成的查询计划在现实中无法执行。



在\ref{fig:c04-write-with-topology}图中我们可以看到网络非对称分区时请求无法执行的问题。




\section{调度阶段的容错设计}



\section{请求失败的容错设计}

\section{容错的熔断和降级措施}


\section{基于拓扑感知的查询和写入规划}
上文提到了集群的拓扑感知，总结来说，ConfigNode Leader不但可以探知每一个节点的存活情况，还能够探测DataNode之间的每一个连接。当发现出现网络分区和节点宕机等情况的时候，本文提出的高可用解决办法可以通过对查询和写入规划的改写和三级重试来做到最大限度的错误避免和自动容错，从而保证用户的请求依然能够完成。本节主要介绍基于拓扑感知的查询流程。



\subsection{基于拓扑感知的写入流程优化}

在2.0.2.1版本之前，对于需要调度器在远端调度的写入碎片实例，如果写入失败，调度器会进行多次重试，直到达到客户端配置的请求超时时间之后才会返回客户端失败。在网络分区的情况下，客户端的请求可能需要在很长时间的重试之后依然失败。如果在客户端侧没有配置重试的策略，那么这就将会导致这种情况下的客户端数据丢失，RPO不为0的情况。

在拓扑感知的情况下，我们采用了立马失败的策略。当我们检测到网络分区判断出本次执行必将失败的时候，我们会直接在调度之前就失败。


在IoTDB的集群中，DataNode3和其他两个节点产生了对称分区问题。此时，如果Client连接上了DataNode3执行写入计划，写入计划最终会被解析为FragmentInstance，对象是Partition1。但是不巧的是，Partition1的两个副本分别在DataNode1和DataNode2上，这导致了这个FragmentInstance需要被调度到这两个节点上执行。
在2.0.2.1方案之前，调度器会先尝试调度到DataNode1上，但是由于网络分区的问题，本次请求最终会以超时失败。接着，调度器会尝试选择第二个副本执行，再次尝试调度，但本次请求最终依然会因为分区而失败告终。调度器会反复重复上述的重试策略，直到本次的执行超过客户端指定的超时时间，最终返回客户端失败。

这种写入计划和执行存在两个问题。首先，对于客户端来说，网络分区的错误需要经历一个完整的超时周期（默认配置是1分钟）才会返回，这会严重影响客户端的吞吐。其次，如果客户端不尝试连接其他的DataNode进行重试，那么本次写入失败最终会导致这部分的数据未能被持久化。


在具备拓扑感知功能后，我们可以在规划阶段就解决这个问题。

详细来说，在生成FragmentInstance之后，我们可以提前根据从ConfigNode Leader下发的集群最新拓扑结构对这个FragemntInstance涉及的Partition的所有地点进行可达性计算。如果有些副本所在的节点因为分区等原因不可达，那么就会标记这些副本，在调度的时候避开这些位置。
如果一个FragmentInstance的所有副本位置都不可达，那么调度器就不会调度，而是直接返回客户端这个FragmentInstance写入失败。客户端此时可以配置重试策略，将本次写入请求连接到DataNode-1或者DataNode-2上进行执行，最终实现写入。
总结来说，在具备拓扑感知能力之后，写入请求不会进行长时间的重试，能够立马发回客户端重试。同时，返回时会带上建议的重试节点，帮助客户端完成最后的写入。





\section{错误时期的三级重试和Failover}

为了确保系统在面临各种故障时仍能保持高可用性和数据一致性，我们采用了一种基于重试的故障转移策略。具体而言，我们设计了一种三级重试机制，该机制涵盖了客户端、协调者以及共识层。通过在这些关键层面上实施重试，我们旨在最大程度地提高错误恢复的成功率，确保即使在不利条件下，系统也能正确处理并解决问题。这种多层次的重试策略不仅增强了系统的鲁棒性，还显著提升了其在复杂分布式环境中的可靠性。

整个三级重试的的实现方案。

\subsection{Session的重定向重试}

在Apache IoTDB中，Session扮演着至关重要的角色，它不仅是客户端应用程序与IoTDB DataNode集群之间进行通信的桥梁，更是实现服务自动发现的关键组件。具体来说，Session封装了与IoTDB服务器建立连接、发送请求、接收响应等一系列底层操作，为应用程序提供了简洁高效的API接口。通过Session，客户端能够轻松地执行数据写入、查询、元数据操作等任务，而无需关心复杂的网络通信细节。此外，Session还具备自动发现DataNode集群服务的能力，这意味着即使集群拓扑结构发生变化，客户端也能自动适应并保持与可用节点的连接，从而确保了系统的高可用性和稳定性。值得注意的是，Session的设计充分考虑了性能和安全性，它通过连接池等机制来减少连接建立的开销，并通过身份验证和权限控制等手段来保障数据的安全性。因此，Session是IoTDB客户端应用程序开发中不可或缺的核心组件。

Session侧的重试是高可用的重要组成部分，是DataNode服务发现和负载均衡的基础。通常来讲，客户端依赖Session连接到集群中的某一个特定DataNode上执行相关的操作请求。然而，如果该DataNode节点突发故障，例如进程宕机、因网络分区而导致连接中断、高负载状态下导致资源耗尽而不能很好地提供服务，此时Session就能通过执行超时和连接通道来检测出异常的发生，并启动故障转移的流程，将请求自动连接到其他的节点中完成。
通过这种无缝切换，Session不但保证了客户端的请求能够被路由到健康的节点上执行，从而保证了数据写入操作的正确性和可靠性，也同样实现了负载均衡的能力。

每一个Session启动的时候，都会配置至少一个DataNode节点和ConfigNode的节点。在Session创建之后，会启动一个后台的进程，这个进程会从集群的大脑ConfigNode中定期拉取所有DataNode的列表和其对应的状态。当当前的DataNode出现问题的时候，Session就会根据列表的顺序不断尝试下一个可用的DataNode。这种实现方式还有一个额外的好处。当集群增加一个新的DataNode的时候，不需要对现在存活的Session进行修改配置并且启动，这个机制能够自动让现有的Session发现这些新增的DataNode，并将数据引导到新的DataNode上。

DataNode在写入的时候，还会进行Redirect的提示，这个行为是集群自动负载均衡的重要能力之一。Session在初始创建的时候，可以默认连接到任何一个DataNode上。当DataNode处理完了这个连接的请求的时候，会根据集群的综合情况考虑，在返回结果的地方增加一个建议重定向的字段，该字段的用意是希望Session后续的连接能够重定向到其他节点上来执行。在处理带有重定向标志的Session的时候，Session会将底层的连接信道重新导引到那个新的DataNode之上，通常，当需要读写的副本的leader节点所在DataNode的时候，才会发生这样的重定向导引活动。

此外，在出现了上述提到的非对称网络分区的时候，查询规划和计划也会建议重定向。如上述所言，如果在非对称分区的时候rootInstance没有地方摆放，那么返回给Session的报错信息就会加上重定向的DataNode，这个DataNode往往能够放置这个root instance。


总结来说，Session侧的重试实现了服务发现、自动转移、负载均衡和容错的能力。



\subsection{协调者}

协调者(Coordinator)是IoTDB的重要组成部分，他负责接受客户端的SQL请求，负责SQL解析、物理计划和分布式执行计划的生成，对每一个FragmentInstance进行调度和执行，以及最终结果的的收集和返回。

协调者侧重试的目的是避免因为单副本、单节点的失效而影响查询和写入的能力。通过充分利用底下共识模块提供的多副本的能力，协调者能够在某个副本写入失败的时候自动转移到另外一个副本上，从而规避系统的暂时性错误。






\subsection{Client的重试策略}






\subsection{熔断和降级措施}




在收集到心跳样本之后，ConfigNode Leader会根据心跳样本的情况对集群的情况做出处理。

ConfigNode首先会对该节点本本身的状态进行研判。在2.0.2.1等先前版本中，如果在一个固定的超时间隔之内（通常是20s）都没有收到DataNode上报的心跳，那么ConfigNode Leader就会研判该节点已经失效，将该节点的状态从RUNNING标记为UNKNOWN。这种基于固定超时时间间隔的算法能够检测出大部分的实际问题，但依然有改进的空间，再下一个小节中我们通过提出基于PhiAccrual的方式对该研判方法进行升级。

ConfigNode其次会根据收集到的Load情况和每一个Region的情况进行相应的研判，从而做出预防性高可用的操作。例如负载均衡、Region迁移、磁盘告警等操作，从而减少错误的产生。


（这部分可以增加一张示意图来描述整一个过程）
（这部分可以详细描述预防性操作的一些行为）